import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
MODEL_NAME = "IlyaGusev/saiga_yandexgpt_8b"
SAVE_PATH = "./models/russian/saiga_yandexgpt_8b"

print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º {MODEL_NAME}...")

try:
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É
    os.makedirs(SAVE_PATH, exist_ok=True)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –¢–µ—Å—Ç–∏—Ä—É–µ–º...")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º –ø—Ä–æ–º–ø—Ç–µ
    prompt = "–°–æ–∑–¥–∞–π –ø–ª–∞–Ω –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            temperature=0.7,
            repetition_penalty=1.2
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{result}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤: {SAVE_PATH}")
    model.save_pretrained(SAVE_PATH)
    tokenizer.save_pretrained(SAVE_PATH)
    
    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    print("–ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")